<?xml version="1.0" encoding="utf-8" ?>
<configuration  xmlns:patch="http://www.sitecore.net/xmlconfig/" xmlns:env="http://www.sitecore.net/xmlconfig/env/"  xmlns:role="http://www.sitecore.net/xmlconfig/role/">
	<sitecore>
		<!--
		
			DO NOT MODIFY THIS FILE.
			
			Use Project-Level patches to override the values here.
			
			-->
		<constellation>
			<robotsTxt>
				<!--
				DEFAULT PROHIBIT CRAWLERS
				
				To prevent a search engine from accidentally indexing a cloud-based lower environment, robots is turned off for all
				roles and environments by default.
				
				-->
				<defaultRules>
					<disallow>/</disallow>
				</defaultRules>
				<!--
				LOCAL DEV ALLOW CRAWLERS
				
				Robots is turned on so developers can see what it looks like on their local environment, which wouldn't be accessible
				to crawlers
				-->
				<defaultRules env:require="Local">
					<allow>/</allow>
				</defaultRules>
				<!--
				PROD CD ALLOW CRAWLERS
				
				Robots is turned on by default on production Content Delivery servers. You can override this with site-specific settings
				-->
				<defaultRules env:require="ProdCD" role:require="ContentDelivery">
					<allow>/</allow>
				</defaultRules>
			</robotsTxt>
		</constellation>
	</sitecore>
</configuration>